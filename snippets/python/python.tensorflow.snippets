snippet script_tensorflow_freeze_model
	import tensorflow as tf
	import os
	import logging


	logging.basicConfig()
	logger = logging.getLogger('freeze')
	logger.setLevel(logging.INFO)


	FLAGS = tf.app.flags.FLAGS
	tf.app.flags.DEFINE_string('checkpoint', 'breakout',
		'checkpoint to freeze')
	tf.app.flags.DEFINE_string('output_nodes', 'train/output/q_values',
		'output nodes to save')
	tf.app.flags.DEFINE_string('output_model', 'breakout.pb',
		'output freezed model')


	def freeze_graph(checkpoint, output_nodes, output_model):
		meta = checkpoint + '.meta'
		data = checkpoint + '.data-00000-of-00001'
		if os.path.isfile(meta) and os.path.isfile(data):
			logger.info('import meta graph...')
			saver = tf.train.import_meta_graph(meta, clear_devices=True)

			config = tf.ConfigProto()
			config.gpu_options.allow_growth = True
			with tf.Session(config=config) as sess:
				logger.info('restoring session...')
				saver.restore(sess, checkpoint)

			logger.info('converting variables to constant...')
			input_graph_def = tf.get_default_graph().as_graph_def()
			nodes = output_nodes.split(',')
			output_graph_def = tf.graph_util.convert_variables_to_constants(
				sess, input_graph_def, nodes)

			with tf.gfile.GFile(output_model, 'wb') as gf:
				logger.info('saving model to %s...', output_model)
				gf.write(output_graph_def.SerializeToString())
		else:
			logger.error('Fail to find %s.', checkpoint)


	def main(_):
		freeze_graph(FLAGS.checkpoint, FLAGS.output_nodes,
			FLAGS.output_model)


	if __name__ == '__main__':
		tf.app.run()
snippet tf_layer_feedforward
	with tf.name_scope('${1:fc1}'):
		$1_size = ${2:256}
		w = tf.get_variable(name='$1_w', shape=[${3:input_size}, $1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
				stddev=np.sqrt(2.0 / $3)))
		b = tf.get_variable(name='$1_b', shape=[$1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.matmul(${4:inputs}, w) + b)
		h = tf.nn.dropout(h, keep_prob)
snippet tf_layer_convolution
	with tf.name_scope('${1:conv1}'):
		$1_size = ${2:32}
		w = tf.get_variable(name='$1_w',
			shape=[${3:5}, $3, ${4:input_channel}, $1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(stddev=${5:0.01}))
		b = tf.get_variable(name='$1_b', shape=[$1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.nn.conv2d(${6:inputs}, w, strides=[1, 1, 1, 1],
			padding='SAME') + b)
		h = tf.nn.max_pool(h, strides=[1, 2, 2, 1], ksize=[1, 2, 2, 1],
			padding='SAME')
		h = tf.nn.dropout(h, keep_prob)
snippet tf_learning rate
	self.learning_rate = tf.Variable(learning_rate, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
snippet tensorflow_parser
	FLAGS = tf.app.flags.FLAGS
	tf.app.flags.DEFINE_bool('saving', False, 'saving model')
	tf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate to train')
	tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size to train')
	tf.app.flags.DEFINE_integer('max_epoch', 10000, 'max epoches to train')
	tf.app.flags.DEFINE_integer('display_epoch', 100, 'epoches to display result')
	tf.app.flags.DEFINE_integer('decay_epoch', 10000,
		'epoches to decay learning rate')
	tf.app.flags.DEFINE_integer('summary_epoch', 10000, 'epoches to save summary')
	tf.app.flags.DEFINE_integer('save_epoch', 1000, 'epoches to save model')
snippet tensorflow_app
	def main(_):
		pass


	if __name__ == '__main__':
		tf.app.run()
snippet tensorflow_argparser
	parser.add_argument('--learning-rate', dest='learning_rate', type=float,
		default=1e-3, help='learning rate for training')
	parser.add_argument('--batch-size', dest='batch_size', type=int,
		default=32, help='batch size for training')
	parser.add_argument('--max-epoches', dest='max_epoches', type=int,
		default=100000, help='max epoches to train')
	parser.add_argument('--display-epoches', dest='display_epoches', type=int,
		default=10, help='epoches to display training result')
	parser.add_argument('--save-epoches', dest='save_epoches', type=int,
		default=10000, help='epoches to save training result')
	parser.add_argument('--summary-epoches', dest='summary_epoches', type=int,
		default=10, help='epoches to save training summary')
	parser.add_argument('--decay-epoches', dest='decay_epoches', type=int,
		default=10000, help='epoches to decay learning rate for training')
	parser.add_argument('--keep-prob', dest='keep_prob', type=float,
		default=0.8, help='keep probability for dropout')
	parser.add_argument('--saving', dest='saving', type=str,
		default='False', help='rather to save the training result')
snippet tensorflow_recurrent
	# inputs
	inputs = []
	labels = []
	for t in range(step_count):
		inputs.append(tf.placeholder(dtype=tf.float32,
			shape=[None, ${1:input_size}]), name='inputs_%d' % t)
		labels.append(tf.placeholder(dtype=tf.float32,
			shape=[None, ${2:output_size}]), name='labels_%d' % t)
		tf.summary.histogram(name='inputs%d' % t, values=inputs[-1])
		tf.summary.histogram(name='labels%d' % t, values=labels[-1])
	keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name='keep_prob')
	# learning rate
	self.learning_rate = tf.Variable(learning_rate, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
	# model
	with tf.name_scope('recurrent'):
		training_state = tf.get_variable(name='training_state', dtype=tf.float32,
			shape=[${4:batch_size}, ${3:hidden_size}], initializer=tf.zeros_initializer(),
			trainable=trainable_state)
		state = tf.get_variable(name='state', dtype=tf.float32,
			shape=[1, $3], initializer=tf.zeros_initializer(),
			trainable=trainable_state)
		iw = tf.get_variable(name='iw', shape=[$1, $3],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $1)))
		ib = tf.get_variable(name='b1', shape=[$3],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		hw = tf.get_variable(name='hw', shape=[$3, $3],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $3)))
		ow = tf.get_variable(name='ow', shape=[$3, $2],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $3)))
		ob = tf.get_variable(name='ob', shape=[$2],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		# forward step function
		def forward(state, input):
			state = tf.nn.relu(tf.matmul(input, iw) + tf.matmul(state, hw) + ib)
			output = tf.matmul(state, ow) + ob
			return tf.nn.dropout(state, keep_prob), output
		# recurrent unit
		training_outputs = []
		outputs = []
		for t in range(step_count):
			if t > 0: tf.get_variable_scope().reuse_variables()
			training_state, training_output = forward(training_state, inputs[t])
			state, output = forward(state, inputs[t])
			training_outputs.append(output)
			outputs.append(output)
			tf.summary.histogram(name='training_outputs%d' % t,
				values=training_output)
			tf.summary.histogram(name='outputs%d' % t,
				values=output)
	# loss and optimizer
	with tf.name_scope('loss'):
		training_outputs = tf.concat(training_outputs, axis=0)
		outputs = tf.concat(outputs, axis=0)
		training_labels = tf.concat(labels, axis=0)
		loss = tf.reduce_mean(tf.pow(training_outputs - training_labels, 2))
		tf.summary.scalar(name='loss', tensor=loss)
	with tf.name_scope('optimizer'):
		optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)
		gvs = optimizer.compute_gradients(loss)
		clipped_gvs = [(tf.clip_by_norm(g, max_grad), v) for g, v in gvs]
		train_op = optimizer.apply_gradients(clipped_gvs)
snippet tensorflow_convolution
	# inputs
	inputs = tf.placeholder(dtype=tf.float32,
		shape=[None, ${1:input_height}, ${2:input_width}, ${3:input_channel}],
		name='input_images')
	labels = tf.placeholder(dtype=tf.float32, shape=[None, ${4:output_size}],
		name='target_labels')
	keep_prob = tf.placeholder(dtype=tf.float32, shape=(),
		name='keep_prob')
	tf.summary.image(name='input_images', tensor=inputs)
	tf.summary.histogram(name='labels', values=labels)
	# learning rate
	self.learning_rate = tf.Variable(${5:learning_rate}, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
	# model
	with tf.name_scope('conv1'):
		h1_size = 32
		w = tf.get_variable(name='conv_w1',
			shape=[5, 5, $3, h1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(stddev=0.006))
		b = tf.get_variable(name='conv_b1', shape=[h1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.nn.conv2d(inputs, w, strides=[1, 1, 1, 1],
			padding='SAME') + b)
		h = tf.nn.max_pool(h, strides=[1, 2, 2, 1], ksize=[1, 2, 2, 1],
			padding='SAME')
		h = tf.nn.dropout(h, keep_prob)
	with tf.name_scope('conv2'):
		h2_size = 32
		w = tf.get_variable(name='conv_w2',
			shape=[5, 5, h1_size, h2_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(stddev=0.01))
		b = tf.get_variable(name='conv_b2', shape=[h2_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.nn.conv2d(h, w, strides=[1, 1, 1, 1],
			padding='SAME') + b)
		h = tf.nn.max_pool(h, strides=[1, 2, 2, 1], ksize=[1, 2, 2, 1],
			padding='SAME')
		h = tf.nn.dropout(h, keep_prob)
	${0}
	with tf.name_scope('fc3'):
		h_shape = h.get_shape().as_list()
		connect_size = h_shape[1] * h_shape[2] * h_shape[3]
		h3_size = 256
		w = tf.get_variable(name='w3',
			shape=[connect_size, h3_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
				stddev=np.sqrt(2.0 / connect_size)))
		b = tf.get_variable(name='b3', shape=[h3_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.matmul(tf.reshape(h, [-1, connect_size]), w) + b)
	with tf.name_scope('output'):
		w = tf.get_variable(name='ow', shape=[h3_size, $4],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
				stddev=np.sqrt(2.0 / h3_size)))
		b = tf.get_variable(name='ob', shape=[$4],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		logits = tf.matmul(h, w) + b
		outputs = tf.nn.softmax(logits)
		tf.summary.histogram(name='outputs', values=outputs)
	# loss and optimizer
	with tf.name_scope('loss'):
		loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
			logits=logits, labels=labels))
		tf.summary.scalar(name='loss', tensor=loss)
		accuracy = tf.reduce_sum(tf.cast(tf.equal(
			tf.argmax(outputs, axis=1), tf.argmax(labels, axis=1)), tf.float32)) * \
			100.0 / tf.cast(tf.shape(labels)[0], tf.float32)
		tf.summary.scalar(name='accuracy', tensor=accuracy)
	with tf.name_scope('optimizer'):
		optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)
		train_op = optimizer.minimize(loss)
snippet tensorflow_object
	class ${1:FeedForwardModel}(object):
		def __init__(self, input_size, output_size,
					model_name='$1',
					learning_rate=${2:1e-3}, decay=${3:0.9}, saving=${4:True}):
			# logger
			logging.basicConfig()
			self.logger = logging.getLogger(model_name)
			self.logger.setLevel(logging.INFO)
			self.logger.info('setting up model...')
			# model
			with tf.variable_scope(model_name):
				self.inputs, self.labels, self.keep_prob, \
					self.outputs, self.loss, self.accuracy, self.train_op = \
					self._build_model(input_size, output_size, learning_rate, decay)
			# checkpoint
			self.start_epoch = 0
			self.saving = saving
			self.saver = tf.train.Saver()
			if saving:
				self.checkpoint_path, summary_path = self._prepare_save_dir(model_name)
				# saver
				self.logger.info('setting up saver...')
				# summary writer
				self.logger.info('setting up summary writer...')
				self.summary_writer = tf.summary.FileWriter(summary_path,
					tf.get_default_graph())
			self.merged_summary = tf.summary.merge_all()

		def _prepare_save_dir(self, model_name):
			index = 0
			while os.path.isdir(model_name + str(index)):
				index += 1
			model_path = model_name + str(index)
			self.logger.info('creating model directory %s...' % (model_path))
			os.mkdir(model_path)
			summary_path = os.path.join(model_path, 'summary')
			os.mkdir(summary_path)
			checkpoint_path = os.path.join(model_path, model_name)
			return checkpoint_path, summary_path

		def _build_model(self, input_size, output_size, learning_rate, decay):
			${0}
			return inputs, labels, keep_prob, outputs, loss, accuracy, train_op

		def load(self, sess, checkpoint_path):
			if os.path.isfile(checkpoint_path + '.meta') and \
					os.path.isfile(checkpoint_path + '.index'):
				self.logger.info('loading last %s checkpoint...' % (checkpoint_path))
				self.saver.restore(sess, checkpoint_path)
				self.start_epoch = int(checkpoint_path.split('-')[-1].strip())
			else:
				self.logger.warning('%s does not exists' % (checkpoint_path))

		def train_batch(self, sess, batch_data, batch_label, keep_prob):
			_, loss = sess.run([self.train_op, self.loss], feed_dict={
				self.inputs: batch_data,
				self.labels: batch_label,
				self.keep_prob: keep_prob
			})
			return loss

		def train(self, sess, data, label,
				batch_size=256, output_period=1000,
				keep_prob=0.8, max_epoch=10000,
				decay_epoch=1000):
			# initialize
			if self.start_epoch == 0:
				self.logger.info('initializing variables...')
				sess.run(tf.global_variables_initializer())
			# training
			self.logger.info('start training...')
			last = time.time()
			for epoch in range(self.start_epoch, self.start_epoch + max_epoch + 1):
				# prepare batch data
				offset = (epoch * batch_size) % (data.shape[0] - batch_size + 1)
				batch_data = data[offset:offset + batch_size, :]
				batch_label = label[offset:offset + batch_size, :]
				loss = self.train_batch(sess, batch_data, batch_label, keep_prob)
				# output
				if epoch % output_period == 0:
					feed_dict = {
						self.inputs: batch_data,
						self.labels: batch_label,
						self.keep_prob: 1.0
					}
					ms, loss, accuracy = sess.run(
						[self.merged_summary, self.loss, self.accuracy], feed_dict)
					self.logger.info('%d. loss: %f | accuracy: %f | time used: %f' %
									(epoch, loss, accuracy, (time.time() - last)))
					last = time.time()
					# save
					if self.saving:
						self.saver.save(sess, self.checkpoint_path, global_step=epoch)
						self.summary_writer.add_summary(ms, global_step=epoch)
				# learning rate decay
				if epoch % decay_epoch == 0 and epoch != 0:
					sess.run(self.decay_lr)

		def predict(self, sess, data):
			return sess.run(self.outputs, feed_dict={
				self.inputs: data,
				self.keep_prob: 1.0
			})


	def test():
		input_size = 16
		output_size = 16

		# prepare fake data
		test_batch_size = 1024
		data = np.random.randn(test_batch_size, input_size)
		label = np.zeros(shape=[test_batch_size, output_size])
		label[:, 0] = 1.0

		model = $1(input_size, output_size,
			model_name='test', saving=False)

		with tf.Session() as sess:
			model.train(sess, data, label,
						batch_size=256,
						output_period=10,
						keep_prob=0.8,
						max_epoch=100)
snippet script_tensorflow_gan
	import tensorflow as tf
	import numpy as np
	import unittest


	class GAN(object):
		def __init__(self, learning_rate):
			self._setup_inputs()
			tf.summary.image('target_images', self.target_images)

			target_labels = tf.one_hot(self.target_labels, 10)
			with tf.variable_scope('generator'):
				self.mnist_generated = \
					self._generator(self.input_noise, target_labels, training=False)
				tf.summary.image('generated_images', self.mnist_generated)

			with tf.variable_scope('generator', reuse=True):
				self.generated = \
					self._generator(self.input_noise, target_labels)

			with tf.variable_scope('discriminator'):
				self.logits, self.outputs = \
					self._discriminator(self.target_images, training=False)

			with tf.variable_scope('discriminator', reuse=True):
				true_logits, _ = self._discriminator(self.target_images)

			with tf.variable_scope('discriminator', reuse=True):
				false_logits, _ = self._discriminator(self.generated)

			with tf.name_scope('loss'):
				self.d_loss1 = tf.reduce_mean(
					tf.nn.sigmoid_cross_entropy_with_logits(logits=true_logits,
					labels=target_labels))
				self.d_loss2 = tf.reduce_mean(
					tf.nn.sigmoid_cross_entropy_with_logits(logits=false_logits,
					labels=tf.zeros_like(target_labels)))
				self.d_loss = tf.add(self.d_loss1, self.d_loss2, name='d_loss')

				self.g_loss = tf.reduce_mean(
					tf.nn.sigmoid_cross_entropy_with_logits(logits=false_logits,
					labels=target_labels), name='g_loss')

			with tf.name_scope('optimization'):
				g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'generator')
				d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'discriminator')

				self.learning_rate = tf.Variable(learning_rate, trainable=False,
					name='learning_rate')
				self.train_g = tf.train.AdamOptimizer(
					self.learning_rate).minimize(self.g_loss, var_list=g_vars)

				self.train_d = tf.train.AdamOptimizer(
					self.learning_rate).minimize(self.d_loss, var_list=d_vars)

			with tf.name_scope('evaluation'):
				self.accuracy = self._evaluate(self.outputs, self.target_labels)
				tf.summary.scalar('train_accuracy', self.accuracy)

			with tf.name_scope('summary'):
				self.summary = tf.summary.merge_all()

		def _setup_inputs(self):
			self.target_images = tf.placeholder(dtype=tf.float32,
				shape=[None, 28, 28, 1], name='target_images')
			self.keep_prob = tf.placeholder(dtype=tf.float32,
				shape=(), name='keep_prob')
			self.target_labels = tf.placeholder(dtype=tf.int32,
				shape=[None], name='target_labels')
			self.input_noise = tf.placeholder(dtype=tf.float32,
				shape=[None, 28, 28, 1], name='input_noise')

		def _deconv(self, inputs, output_size, kernel_size, stride,
				leak=0.1, training=True):
			deconv = tf.layers.conv2d_transpose(inputs, output_size,
				kernel_size, strides=stride,
				activation=lambda x: tf.nn.leaky_relu(x, leak))
			if training:
				deconv = tf.nn.dropout(deconv, keep_prob=self.keep_prob)
			return deconv

		def _block(self, inputs, output_size, leak=0.1, training=True):
			init = tf.variance_scaling_initializer(2.0)

			conv = tf.layers.conv2d(inputs, output_size, 3,
				activation=lambda x: tf.nn.leaky_relu(x, leak), padding='SAME',
				kernel_initializer=init)
			conv = tf.layers.conv2d(conv, output_size, 1,
				activation=lambda x: tf.nn.leaky_relu(x, leak), padding='SAME',
				kernel_initializer=init)

			if training:
				conv = tf.nn.dropout(conv, keep_prob=self.keep_prob)
			return conv

		def _stack(self, conv, output_size, leak=0.1, training=True, multiple=1):
			for _ in range(multiple):
				conv = self._block(conv, output_size, leak, training=training)
			return conv

		def _down_sample(self, inputs, output_size, kernel_size,
				leak=0.1, training=True):
			init = tf.variance_scaling_initializer(2.0)
			conv = tf.layers.conv2d(inputs, output_size, kernel_size, strides=2,
				activation=lambda x: tf.nn.leaky_relu(x, leak), padding='SAME',
				kernel_initializer=init)

			if training:
				conv = tf.nn.dropout(conv, keep_prob=self.keep_prob)
			return conv

		def _generator(self, input_noise, label, training=True):
			with tf.name_scope('reshape_label'):
				label_input = tf.reshape(label, [-1, 1, 1, 10])

			with tf.name_scope('deconv1'):
				deconv = self._deconv(label_input, 16, 8, 1, training=training)

			with tf.name_scope('deconv2'):
				deconv = self._deconv(deconv, 32, 8, 2, training=training)

			with tf.name_scope('deconv3'):
				deconv = self._deconv(deconv, 64, 5, 1, training=training)

			with tf.name_scope('deconv4'):
				deconv = self._deconv(deconv, 128, 3, 1, training=training)

			init = tf.variance_scaling_initializer(2.0)
			with tf.name_scope('conv5'):
				conv1 = tf.layers.conv2d(deconv, 128, 3,
					activation=lambda x: tf.nn.leaky_relu(x, 0.1), padding='SAME',
					kernel_initializer=init)
				conv2 = tf.layers.conv2d(input_noise, 128, 3,
					activation=lambda x: tf.nn.leaky_relu(x, 0.1), padding='SAME',
					kernel_initializer=init)
				conv = tf.concat([conv1, conv2], axis=3)
				conv = self._block(conv, 256, training=training)

			with tf.name_scope('conv6'):
				conv = self._block(conv, 256, training=training)

			with tf.name_scope('output'):
				ow = tf.get_variable('ow', shape=[3, 3, 256, 1],
					initializer=init)
				ob = tf.get_variable('ob', shape=[1],
					initializer=tf.zeros_initializer())
				logits = tf.add(
					tf.nn.conv2d(conv, ow, strides=[1, 1, 1, 1], padding='SAME'), ob,
					name='logits')
			return logits

		def _discriminator(self, input_images, training=True):
			with tf.name_scope('conv1'):
				init = tf.variance_scaling_initializer(2.0)
				#  init = tf.random_normal_initializer(stddev=0.01)
				conv = tf.layers.conv2d(input_images, 32, 3,
					activation=lambda x: tf.nn.leaky_relu(x, 0.1), padding='SAME',
					kernel_initializer=init)

			with tf.name_scope('conv2'):
				conv = self._block(conv, 64, training=training)
				conv = self._down_sample(conv, 64, 3, training=training)

			with tf.name_scope('conv3'):
				conv = self._stack(conv, 128, training=training, multiple=2)
				conv = self._down_sample(conv, 128, 3, training=training)

			with tf.name_scope('conv4'):
				conv = self._stack(conv, 256, training=training, multiple=2)

			with tf.name_scope('conv5'):
				conv = self._stack(conv, 512, training=training, multiple=3)

			with tf.name_scope('conv6'):
				conv = self._block(conv, 1024, training=training)

			with tf.name_scope('output'):
				init = tf.variance_scaling_initializer(2.0)
				ow = tf.get_variable('ow', shape=[7, 7, 1024, 10],
					initializer=init)
				ob = tf.get_variable('ob', shape=[10],
					initializer=tf.zeros_initializer())
				logits = tf.nn.conv2d(conv, ow, strides=[1, 1, 1, 1], padding='VALID') + ob
				logits = tf.reshape(logits, [-1, 10], name='logits')
				outputs = tf.nn.sigmoid(logits)
			return logits, outputs

		def _evaluate(self, prediction, labels):
			eq = tf.cast(tf.equal(tf.cast(tf.argmax(prediction, axis=1), tf.int32),
				labels), tf.float32)
			return tf.reduce_mean(eq)


	class TestGAN(unittest.TestCase):
		@classmethod
		def setUpClass(cls):
			cls.gan = GAN(1e-3)
			config = tf.ConfigProto()
			config.gpu_options.allow_growth = True
			cls.sess = tf.Session(config=config)
			cls.sess.run(tf.global_variables_initializer())

		@classmethod
		def tearDownClass(cls):
			cls.sess.close()

		def test_generator_output(self):
			labels = np.random.randint(0, 9, [1])
			noise = np.random.randn(1, 28, 28, 1)

			gen = self.sess.run(self.gan.mnist_generated, feed_dict={
				self.gan.target_labels: labels,
				self.gan.input_noise: noise,
			})
			print(np.mean(gen))
			print(np.std(gen))

		def test_discriminator_outptt(self):
			images = np.random.rand(1, 28, 28, 1)

			logits, outputs = self.sess.run([self.gan.logits, self.gan.outputs],
				feed_dict={
					self.gan.target_images: images,
				})
			print(np.mean(logits))
			print(np.std(logits))

			print(np.mean(outputs))
			print(np.std(outputs))


	if __name__ == '__main__':
		unittest.main()
snippet def_tf_load_graph
	def load_graph(pb_file):
		with tf.gfile.GFile(pb_file, 'rb') as f:
			graph_def = tf.GraphDef()
			graph_def.ParseFromString(f.read())

		with tf.Graph().as_default() as g:
			tf.import_graph_def(graph_def)
		return g
snippet import_tf
	import tensorflow as tf
	import tensorflow.keras.backend as K
	import tensorflow.keras.layers as layers
	from tensorflow.keras.models import Model, Sequential
	from tensorflow.data import Dataset

snippet import_ml
	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	import seaborn as sns

snippet class_keras_layer
	class ${1:MyLayer}(tf.keras.layers.Layer):
		def __init__(self, **kwargs):
			super($1, self).__init__(**kwargs)
			${2:## code}

		def build(self, input_shape):
			${3:## code}
			super($1, self).build(input_shape)

		def call(self, x):
			return x

		def compute_output_shape(self, input_shape):
			return input_shape
snippet class_keras_relu_layer
	class ${1:ReLUConv2D}(tf.keras.layers.Layer):
		def __init__(self, output_size, ksize, stride, **kwargs):
			super($1, self).__init__(**kwargs)
			self.output_size = output_size
			self.ksize = ksize
			self.stride = stride

		def build(self, input_shape):
			self.conv = tf.keras.layers.Conv2D(self.output_size,
				self.ksize, self.stride, padding='same')
			self.norm = tf.keras.layers.BatchNormalization()
			self.relu = tf.keras.layers.ReLU()
			super($1, self).build(input_shape)

		def call(self, x):
			conv = self.conv(x)
			norm = self.norm(conv)
			return self.relu(norm)

		def compute_output_shape(self, input_shape):
			return (input_shape[0],
				input_shape[1] / self.stride,
				input_shape[2] / self.stride,
				self.output_size)
snippet class_keras_pool
	class ${1:Pool}(layers.Layer):
		def __init__(self, output_size, ksize, **kwargs):
			super($1, self).__init__(**kwargs)
			self.output_size = output_size
			self.ksize = ksize

		def build(self, input_shape):
			self.conv = layers.Conv2D(self.output_size, self.ksize, 1,
				padding='valid')
			self.norm = layers.BatchNormalization()
			self.relu = layers.ReLU()
			super($1, self).build(input_shape)

		def call(self, x):
			conv = self.conv(x)
			norm = self.norm(conv)
			return self.relu(norm)

		def compute_output_shape(self, input_shape):
			return (input_shape[0], 1, 1, self.output_size)
