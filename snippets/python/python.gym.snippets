snippet import_gym
	import gym
	from gym import Wrapper

snippet class_gym_wrapper
	class ${1:MyWrapper}(gym.Wrapper):
		def __init__(self, env):
			super($1, self).init(env)
			print(self.metadata)
			print(self.action_space)
			print(self.observation_space)
			print(self.reward_range)
			print(self.spec)

		def _step(self, action):
			${2:# code...}
			return self.env.step(action)

		def _reset(self):
			${3:# code...}
			return self.env.reset()

		def _render(self):
			return self.env.render()

		def _close(self):
			return self.env.close()

		def _seed(self, seed=None):
			return self.env.seed(seed)

snippet class_gym_reset_wrapper
	class ${1:ResetEnv}(gym.Wrapper):
		def __init__(self, env):
			super($1, self).init(env)
			${2:# code...}

		def reset(self):
			self.env.reset()
			state, _, done, _ = self.env.step(1)
			if done: self.env.reset()
			state, _, done, _ = self.env.step(2)
			if done: self.env.reset()
			return state

		def step(self, action):
			return self.env.step(action)

snippet class_gym_episodic_life_wrapper
	class ${1:EpisodicLifeEnv}(gym.Wrapper):
		def __init__(self, env):
			super($1, self).init(env)
			self.lives = 0
			self.was_real_done = True

		def step(self, action):
			state, reward, done, info = self.env.step(action)
			lives = info['ale.lives']
			if lives < self.lives:
				done = True
			self.lives = lives
			return state, reward, done, info

		def reset(self):
			if self.lives == 0:
				state = self.env.reset()
			else:
				state, _, _, info = self.env.step(0)
				self.lives = info['ale.lives']
			return state

snippet class_gym_image_state_wrapper
	class ${1:ImageObservationWrapper}(gym.ObservationWrapper):
		def __init__(self, env, w, h):
			super($1, self).init(env)
			self.w, self.h = w, h
			self.observation_space = gym.spaces.Box(
				low=0, high=255, shape=(self.h, self.w))

		def _observation(self, obs):
			return self.process_image(obs, self.w, self.h)

		def process_image(self, state, input_width, input_height):
			# cropping important area of the state
			image = Image.fromarray(state).crop([0, 0, 100, 100])
			image = image.resize([input_width, input_height], Image.NEAREST).convert('L')
			img = np.array(image, dtype=np.uint8)
			return img

snippet class_gym_fire_live_loss_wrapper
	class ${1:FireLiveLoss}(gym.Wrapper):
		def __init__(self, env):
			super($1, self).init(env)
			self.lives = 0

		def _step(self, action):
			state, reward, done, info = self.env.step(action)
			lives = info['ale.lives']
			if lives < self.lives:
				state, _, _, info = self.env.step(0)
				state, _, _, info = self.env.step(1)
			self.lives = lives
			return state, reward, done, info

snippet gym_history_state_wrapper
	class ${1:HistoryStatesEnv}(gym.Wrapper):
		def __init__(self, env, history_size):
			super($1, self).init(env)
			self.history = deque(maxlen=history_size)

		def observation(self, obs):
			return self.env.observation(obs)

		def _reset(self):
			state = self.env.reset()
			empty = np.zeros(self.observation_space.shape, np.uint8)
			for _ in range(self.history.maxlen - 1):
				self.history.append(empty)
				self.history.append(state)
			return self._history()

		def _history(self):
			return np.stack(self.history, axis=2)

		def _step(self, action):
			state, reward, done, info = self.env.step(action)
			self.history.append(state)
			return self._history(), reward, done, info

snippet script_gym_atari_environment
	import gym
	import numpy as np
	import time
	from PIL import Image
	from collections import deque


	class FireResetEnv(gym.Wrapper):
		def __init__(self, env):
			gym.Wrapper.__init__(self, env)
			assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
			assert len(env.unwrapped.get_action_meanings()) >= 3

		def reset(self):
			self.env.reset()
			state, _, done, _ = self.env.step(1)
			if done: self.env.reset()
			state, _, done, _ = self.env.step(2)
			if done: self.env.reset()
			return state

		def step(self, action):
			return self.env.step(action)


	class EpisodicLifeEnv(gym.Wrapper):
		def __init__(self, env):
			gym.Wrapper.__init__(self, env)
			self.lives = 0
			self.was_real_done = True

		def step(self, action):
			state, reward, done, info = self.env.step(action)
			lives = info['ale.lives']
			if lives < self.lives:
				done = True
			self.lives = lives
			return state, reward, done, info

		def reset(self):
			if self.lives == 0:
				state = self.env.reset()
			else:
				state, _, _, info = self.env.step(0)
				self.lives = info['ale.lives']
			return state


	class MapState(gym.ObservationWrapper):
		def __init__(self, env, w, h):
			gym.ObservationWrapper.__init__(self, env)
			self.w, self.h = w, h
			self.observation_space = gym.spaces.Box(
				low=0, high=255, shape=(self.h, self.w))

		def _observation(self, obs):
			return self.process_image(obs, self.w, self.h)

		def process_image(self, state, input_width, input_height):
			# cropping important area of the state
			image = Image.fromarray(state).crop([0, 0, 100, 100])
			image = image.resize([input_width, input_height], Image.NEAREST).convert('L')
			img = np.array(image, dtype=np.uint8)
			return img


	class FireLiveLoss(gym.Wrapper):
		def __init__(self, env):
			gym.Wrapper.__init__(self, env)
			self.lives = 0

		def step(self, action):
			state, reward, done, info = self.env.step(action)
			lives = info['ale.lives']
			if lives < self.lives:
				state, _, _, info = self.env.step(0)
				state, _, _, info = self.env.step(1)
			self.lives = lives
			return state, reward, done, info


	class HistoryStatesEnv(gym.Wrapper):
		def __init__(self, env, history_size):
			gym.Wrapper.__init__(self, env)
			self.history = deque(maxlen=history_size)

		def observation(self, obs):
			return self.env.observation(obs)

		def reset(self):
			state = self.env.reset()
			empty = np.zeros(self.observation_space.shape, np.uint8)
			for _ in range(self.history.maxlen - 1):
				self.history.append(empty)
				self.history.append(state)
			return self._history()

		def _history(self):
			return np.stack(self.history, axis=2)

		def step(self, action):
			state, reward, done, info = self.env.step(action)
			self.history.append(state)
			return self._history(), reward, done, info


	def get_training_env(name, w, h):
		env = gym.make(name)
		env.seed((int(time.time())))
		env = EpisodicLifeEnv(env)
		env = FireResetEnv(env)
		env = MapState(env, w, h)
		return env


	def get_test_env(name, w, h, history_size):
		env = gym.make(name)
		env.seed((int(time.time())))
		env = FireLiveLoss(env)
		env = MapState(env, w, h)
		env = HistoryStatesEnv(env, history_size)
		return env
