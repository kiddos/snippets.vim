snippet #!
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-
snippet #!3
	#!/usr/bin/env python3
	# -*- coding: utf-8 -*-
snippet unicode
	def __unicode__(self):
		${0:representation}
# Module Docstring
snippet docs
	"""
	File: ${1:`vim_snippets#Filename('$1.py', 'foo.py')`}
	Author: `g:snips_author`
	Email: `g:snips_email`
	Github: `g:snips_github`
	Description: ${0}
	"""

# New Property
snippet property
	def ${1:foo}():
		doc = "${2:The $1 property.}"
		def fget(self):
			${3:return self._$1}
		def fset(self, value):
			${4:self._$1 = value}
		def fdel(self):
			${0:del self._$1}
		return locals()
	$1 = property(**$1())
# Encodes
snippet encoding utf8
	# -*- coding: utf-8 -*-
snippet encoding latin1
	# -*- coding: latin-1 -*-
snippet encoding ascii
	# -*- coding: ascii -*-
# if __name__ == '__main__':
snippet ifmain
	if __name__ == '__main__':
		${0:main()}
# pdb in nosetests
snippet nosetrace
	from nose.tools import set_trace
	set_trace()
snippet pprint
	import pprint
	pprint.pprint(${1})
# logging
snippet logging
	import logging
	logger = logging.getLogger(${0:__name__})
snippet logger
	logging.basicConfig()
	logger = logging.getLogger(${1:__name__})
	logger.setLevel(${2:logging.INFO})
	${0}
# conflict with lambda=ld, therefor we change into Logger.debuG
snippet epydoc
	"""${1:Description}

	@param ${2:param}: ${3: Description}
	@type  $2: ${4: Type}

	@return: ${5: Description}
	@rtype : ${6: Type}

	@raise e: ${0: Description}
	"""
# sqlite3
snippet sqlite3_connection
	connection = sqlite3.connect(${1:path})
	cursor = connection.cursor()
	${0}
	connection.commit()
	connection.close()
# argument parser
snippet argparse
	from argparse import ArgumentParser

	parser = ArgumentParser()
	${0}
	args = parser.parse_args()
## defined classes template
snippet class_unit_test
	class TestName(unittest.TestCase):
		def test_case1(self):
			self.assertTrue(True)
snippet class_gym_environment
	class CustomEnvironment(Env):
		metadata = {'render.modes': ['ansi', 'rgb_array', 'human']}

		def __init__(self, path):
			self.current_dir = os.path.dirname(os.path.realpath(__file__))
			self._reset()
			logging.basicConfig()
			self.logger = logging.getLogger(__name__)
			self.logger.setLevel(logging.INFO)
			self.spec = None
			self.view = None
			self.action_space = spaces.Discrete(2)
			self.observation_space = spaces.Box(low=0, high=1, shape=(1, 1))

		def get_state(self):
			return np.zeros(shape=[1, 1])

		def _seed(self, seed=None):
			pass

		def _step(self, action):
			info = {}
			reward = 0
			done = False
			return self.get_state(), reward, done, info

		def _reset(self):
			return self.get_state()

		def _render(self, mode='human', close=False):
			if not close:
				if mode == 'rgb_array':
					return self.get_state()
				elif mode is 'human':
					pass
				else:
					self.logger.info('\n%s\n' % (str(self.get_state())))

		def _close(self):
			pass
##
## Serial
snippet serial
	import serial


	s = serial.Serial('${1:/dev/ttyUSB0}', ${2:9600},
		parity=${2:serial.PARITY_EVEN},
		stopbits=${3:serial.STOPBITS_ONE},
		timeout=${4:1000})
##
## Tensorflow Framework
snippet tensorflow_object
	class ${1:FeedForwardModel}(object):
		def __init__(self, input_size, output_size,
					model_name='$1',
					learning_rate=${2:1e-3}, decay=${3:0.9}, saving=${4:True}):
			# logger
			logging.basicConfig()
			self.logger = logging.getLogger(model_name)
			self.logger.setLevel(logging.INFO)
			self.logger.info('setting up model...')
			# model
			with tf.variable_scope(model_name):
				self.inputs, self.labels, self.keep_prob, \
					self.outputs, self.loss, self.accuracy, self.train_op = \
					self._build_model(input_size, output_size, learning_rate, decay)
			# checkpoint
			self.start_epoch = 0
			self.saving = saving
			self.saver = tf.train.Saver()
			if saving:
				self.checkpoint_path, summary_path = self._prepare_save_dir(model_name)
				# saver
				self.logger.info('setting up saver...')
				# summary writer
				self.logger.info('setting up summary writer...')
				self.summary_writer = tf.summary.FileWriter(summary_path,
					tf.get_default_graph())
			self.merged_summary = tf.summary.merge_all()

		def _prepare_save_dir(self, model_name):
			index = 0
			while os.path.isdir(model_name + str(index)):
				index += 1
			model_path = model_name + str(index)
			self.logger.info('creating model directory %s...' % (model_path))
			os.mkdir(model_path)
			summary_path = os.path.join(model_path, 'summary')
			os.mkdir(summary_path)
			checkpoint_path = os.path.join(model_path, model_name)
			return checkpoint_path, summary_path

		def _build_model(self, input_size, output_size, learning_rate, decay):
			${0}
			return inputs, labels, keep_prob, outputs, loss, accuracy, train_op

		def load(self, sess, checkpoint_path):
			if os.path.isfile(checkpoint_path + '.meta') and \
					os.path.isfile(checkpoint_path + '.index'):
				self.logger.info('loading last %s checkpoint...' % (checkpoint_path))
				self.saver.restore(sess, checkpoint_path)
				self.start_epoch = int(checkpoint_path.split('-')[-1].strip())
			else:
				self.logger.warning('%s does not exists' % (checkpoint_path))

		def train_batch(self, sess, batch_data, batch_label, keep_prob):
			_, loss = sess.run([self.train_op, self.loss], feed_dict={
				self.inputs: batch_data,
				self.labels: batch_label,
				self.keep_prob: keep_prob
			})
			return loss

		def train(self, sess, data, label,
				batch_size=256, output_period=1000,
				keep_prob=0.8, max_epoch=10000,
				decay_epoch=1000):
			# initialize
			if self.start_epoch == 0:
				self.logger.info('initializing variables...')
				sess.run(tf.global_variables_initializer())
			# training
			self.logger.info('start training...')
			last = time.time()
			for epoch in range(self.start_epoch, self.start_epoch + max_epoch + 1):
				# prepare batch data
				offset = (epoch * batch_size) % (data.shape[0] - batch_size + 1)
				batch_data = data[offset:offset + batch_size, :]
				batch_label = label[offset:offset + batch_size, :]
				loss = self.train_batch(sess, batch_data, batch_label, keep_prob)
				# output
				if epoch % output_period == 0:
					feed_dict = {
						self.inputs: batch_data,
						self.labels: batch_label,
						self.keep_prob: 1.0
					}
					ms, loss, accuracy = sess.run(
						[self.merged_summary, self.loss, self.accuracy], feed_dict)
					self.logger.info('%d. loss: %f | accuracy: %f | time used: %f' %
									(epoch, loss, accuracy, (time.time() - last)))
					last = time.time()
					# save
					if self.saving:
						self.saver.save(sess, self.checkpoint_path, global_step=epoch)
						self.summary_writer.add_summary(ms, global_step=epoch)
				# learning rate decay
				if epoch % decay_epoch == 0 and epoch != 0:
					sess.run(self.decay_lr)

		def predict(self, sess, data):
			return sess.run(self.outputs, feed_dict={
				self.inputs: data,
				self.keep_prob: 1.0
			})


	def test():
		input_size = 16
		output_size = 16

		# prepare fake data
		test_batch_size = 1024
		data = np.random.randn(test_batch_size, input_size)
		label = np.zeros(shape=[test_batch_size, output_size])
		label[:, 0] = 1.0

		model = $1(input_size, output_size,
			model_name='test', saving=False)

		with tf.Session() as sess:
			model.train(sess, data, label,
						batch_size=256,
						output_period=10,
						keep_prob=0.8,
						max_epoch=100)
snippet tensorflow_feedforward
	# inputs
	inputs = tf.placeholder(dtype=tf.float32, shape=[None, ${1:input_size}],
		name='inputs')
	labels = tf.placeholder(dtype=tf.float32, shape=[None, ${2:output_size}],
		name='labels')
	keep_prob = tf.placeholder(dtype=tf.float32, shape=(),
		name='keep_prob')
	tf.summary.histogram(name='inputs', values=inputs)
	tf.summary.histogram(name='labels', values=labels)
	# learning rate
	self.learning_rate = tf.Variable(${3:learning_rate}, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
	# model
	with tf.name_scope('fc1'):
		h1_size = 256
		w = tf.get_variable(name='fc1_w', shape=[$1, h1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $1)))
		b = tf.get_variable(name='fc1_b', shape=[h1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.matmul(inputs, w) + b)
		h = tf.nn.dropout(h, keep_prob)
	${0}
	with tf.name_scope('output'):
		w = tf.get_variable(name='ow', shape=[h1_size, $2],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / h1_size)))
		b = tf.get_variable(name='ob', shape=[$2],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		logits = tf.matmul(h, w) + b
		outputs = tf.nn.softmax(logits)
		tf.summary.histogram(name='outputs', values=outputs)
	# loss and optimizer
	with tf.name_scope('loss'):
		loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
			logits=logits, labels=labels))
		tf.summary.scalar(name='loss', tensor=loss)
		accuracy = tf.reduce_sum(tf.cast(tf.equal(
			tf.argmax(outputs, axis=1), tf.argmax(labels, axis=1)), tf.float32)) * \
			100.0 / tf.cast(tf.shape(labels)[0], tf.float32)
		tf.summary.scalar(name='accuracy', tensor=accuracy)
	with tf.name_scope('optimizer'):
		optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)
		train_op = optimizer.minimize(loss)
snippet tensorflow_convolution
	# inputs
	inputs = tf.placeholder(dtype=tf.float32,
		shape=[None, ${1:input_height}, ${2:input_width}, ${3:input_channel}],
		name='input_images')
	labels = tf.placeholder(dtype=tf.float32, shape=[None, ${4:output_size}],
		name='target_labels')
	keep_prob = tf.placeholder(dtype=tf.float32, shape=(),
		name='keep_prob')
	tf.summary.image(name='input_images', tensor=inputs)
	tf.summary.histogram(name='labels', values=labels)
	# learning rate
	self.learning_rate = tf.Variable(${5:learning_rate}, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
	# model
	with tf.name_scope('conv1'):
		h1_size = 32
		w = tf.get_variable(name='conv_w1',
			shape=[5, 5, $3, h1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(stddev=0.006))
		b = tf.get_variable(name='conv_b1', shape=[h1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.nn.conv2d(inputs, w, strides=[1, 1, 1, 1],
			padding='SAME') + b)
		h = tf.nn.max_pool(h, strides=[1, 2, 2, 1], ksize=[1, 2, 2, 1],
			padding='SAME')
		h = tf.nn.dropout(h, keep_prob)
	with tf.name_scope('conv2'):
		h2_size = 32
		w = tf.get_variable(name='conv_w2',
			shape=[5, 5, h1_size, h2_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(stddev=0.01))
		b = tf.get_variable(name='conv_b2', shape=[h2_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.nn.conv2d(h, w, strides=[1, 1, 1, 1],
			padding='SAME') + b)
		h = tf.nn.max_pool(h, strides=[1, 2, 2, 1], ksize=[1, 2, 2, 1],
			padding='SAME')
		h = tf.nn.dropout(h, keep_prob)
	${0}
	with tf.name_scope('fc3'):
		h_shape = h.get_shape().as_list()
		connect_size = h_shape[1] * h_shape[2] * h_shape[3]
		h3_size = 256
		w = tf.get_variable(name='w3',
			shape=[connect_size, h3_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
				stddev=np.sqrt(2.0 / connect_size)))
		b = tf.get_variable(name='b3', shape=[h3_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.matmul(tf.reshape(h, [-1, connect_size]), w) + b)
	with tf.name_scope('output'):
		w = tf.get_variable(name='ow', shape=[h3_size, $4],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
				stddev=np.sqrt(2.0 / h3_size)))
		b = tf.get_variable(name='ob', shape=[$4],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		logits = tf.matmul(h, w) + b
		outputs = tf.nn.softmax(logits)
		tf.summary.histogram(name='outputs', values=outputs)
	# loss and optimizer
	with tf.name_scope('loss'):
		loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(
			logits=logits, labels=labels))
		tf.summary.scalar(name='loss', tensor=loss)
		accuracy = tf.reduce_sum(tf.cast(tf.equal(
			tf.argmax(outputs, axis=1), tf.argmax(labels, axis=1)), tf.float32)) * \
			100.0 / tf.cast(tf.shape(labels)[0], tf.float32)
		tf.summary.scalar(name='accuracy', tensor=accuracy)
	with tf.name_scope('optimizer'):
		optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)
		train_op = optimizer.minimize(loss)
snippet tensorflow_recurrent
	# inputs
	inputs = []
	labels = []
	for t in range(step_count):
		inputs.append(tf.placeholder(dtype=tf.float32,
			shape=[None, ${1:input_size}]), name='inputs_%d' % t)
		labels.append(tf.placeholder(dtype=tf.float32,
			shape=[None, ${2:output_size}]), name='labels_%d' % t)
		tf.summary.histogram(name='inputs%d' % t, values=inputs[-1])
		tf.summary.histogram(name='labels%d' % t, values=labels[-1])
	keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name='keep_prob')
	# learning rate
	self.learning_rate = tf.Variable(learning_rate, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
	# model
	with tf.name_scope('recurrent'):
		training_state = tf.get_variable(name='training_state', dtype=tf.float32,
			shape=[${4:batch_size}, ${3:hidden_size}], initializer=tf.zeros_initializer(),
			trainable=trainable_state)
		state = tf.get_variable(name='state', dtype=tf.float32,
			shape=[1, $3], initializer=tf.zeros_initializer(),
			trainable=trainable_state)
		iw = tf.get_variable(name='iw', shape=[$1, $3],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $1)))
		ib = tf.get_variable(name='b1', shape=[$3],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		hw = tf.get_variable(name='hw', shape=[$3, $3],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $3)))
		ow = tf.get_variable(name='ow', shape=[$3, $2],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
			stddev=np.sqrt(2.0 / $3)))
		ob = tf.get_variable(name='ob', shape=[$2],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		# forward step function
		def forward(state, input):
			state = tf.nn.relu(tf.matmul(input, iw) + tf.matmul(state, hw) + ib)
			output = tf.matmul(state, ow) + ob
			return tf.nn.dropout(state, keep_prob), output
		# recurrent unit
		training_outputs = []
		outputs = []
		for t in range(step_count):
			if t > 0: tf.get_variable_scope().reuse_variables()
			training_state, training_output = forward(training_state, inputs[t])
			state, output = forward(state, inputs[t])
			training_outputs.append(output)
			outputs.append(output)
			tf.summary.histogram(name='training_outputs%d' % t,
				values=training_output)
			tf.summary.histogram(name='outputs%d' % t,
				values=output)
	# loss and optimizer
	with tf.name_scope('loss'):
		training_outputs = tf.concat(training_outputs, axis=0)
		outputs = tf.concat(outputs, axis=0)
		training_labels = tf.concat(labels, axis=0)
		loss = tf.reduce_mean(tf.pow(training_outputs - training_labels, 2))
		tf.summary.scalar(name='loss', tensor=loss)
	with tf.name_scope('optimizer'):
		optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)
		gvs = optimizer.compute_gradients(loss)
		clipped_gvs = [(tf.clip_by_norm(g, max_grad), v) for g, v in gvs]
		train_op = optimizer.apply_gradients(clipped_gvs)
snippet tf_layer_feedforward
	with tf.name_scope('${1:fc1}'):
		$1_size = ${2:256}
		w = tf.get_variable(name='$1_w', shape=[${3:input_size}, $1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(
				stddev=np.sqrt(2.0 / $3)))
		b = tf.get_variable(name='$1_b', shape=[$1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.matmul(${4:inputs}, w) + b)
		h = tf.nn.dropout(h, keep_prob)
snippet tf_layer_convolution
	with tf.name_scope('${1:conv1}'):
		$1_size = ${2:32}
		w = tf.get_variable(name='$1_w',
			shape=[${3:5}, $3, ${4:input_channel}, $1_size],
			dtype=tf.float32,
			initializer=tf.random_normal_initializer(stddev=${5:0.01}))
		b = tf.get_variable(name='$1_b', shape=[$1_size],
			dtype=tf.float32,
			initializer=tf.constant_initializer(value=1e-3))
		h = tf.nn.relu(tf.nn.conv2d(${6:inputs}, w, strides=[1, 1, 1, 1],
			padding='SAME') + b)
		h = tf.nn.max_pool(h, strides=[1, 2, 2, 1], ksize=[1, 2, 2, 1],
			padding='SAME')
		h = tf.nn.dropout(h, keep_prob)
snippet tf_learning rate
	self.learning_rate = tf.Variable(learning_rate, name='learning_rate',
		trainable=False)
	self.decay_lr = tf.assign(self.learning_rate, self.learning_rate * decay)
	tf.summary.scalar(name='learning_rate', tensor=self.learning_rate)
snippet tensorflow_parser
	FLAGS = tf.app.flags.FLAGS
	tf.app.flags.DEFINE_bool('saving', False, 'saving model')
	tf.app.flags.DEFINE_float('learning_rate', 1e-3, 'learning rate to train')
	tf.app.flags.DEFINE_integer('batch_size', 64, 'batch size to train')
	tf.app.flags.DEFINE_integer('max_epoch', 10000, 'max epoches to train')
	tf.app.flags.DEFINE_integer('display_epoch', 100, 'epoches to display result')
	tf.app.flags.DEFINE_integer('decay_epoch', 10000,
		'epoches to decay learning rate')
	tf.app.flags.DEFINE_integer('summary_epoch', 10000, 'epoches to save summary')
	tf.app.flags.DEFINE_integer('save_epoch', 1000, 'epoches to save model')
snippet tensorflow_app
	def main(_):
		pass


	if __name__ == '__main__':
		tf.app.run()
snippet tensorflow_argparser
	parser.add_argument('--learning-rate', dest='learning_rate', type=float,
		default=1e-3, help='learning rate for training')
	parser.add_argument('--batch-size', dest='batch_size', type=int,
		default=32, help='batch size for training')
	parser.add_argument('--max-epoches', dest='max_epoches', type=int,
		default=100000, help='max epoches to train')
	parser.add_argument('--display-epoches', dest='display_epoches', type=int,
		default=10, help='epoches to display training result')
	parser.add_argument('--save-epoches', dest='save_epoches', type=int,
		default=10000, help='epoches to save training result')
	parser.add_argument('--summary-epoches', dest='summary_epoches', type=int,
		default=10, help='epoches to save training summary')
	parser.add_argument('--decay-epoches', dest='decay_epoches', type=int,
		default=10000, help='epoches to decay learning rate for training')
	parser.add_argument('--keep-prob', dest='keep_prob', type=float,
		default=0.8, help='keep probability for dropout')
	parser.add_argument('--saving', dest='saving', type=str,
		default='False', help='rather to save the training result')
##
## ROS
snippet ros_publisher
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-

	import rospy
	from std_msgs.msg import String


	def talker():
		pub = rospy.Publisher('${2:topic}', String, queue_size=100)
		rospy.init_node('${1:node_name}', anonymous=True)
		rate = rospy.Rate(10)
		while not rospy.is_shutdown():
			msg = 'message [%s]' % rospy.get_time()
			rospy.loginfo(msg)
			pub.publish(msg)
			rate.sleep()


	if __name__ == '__main__':
		try:
			talker()
		except rospy.ROSInterruptException:
			pass
snippet ros_subscriber
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-

	import rospy
	from std_msgs.msg import String


	def listen_callback(data):
		rospy.loginfo(rospy.get_caller_id() + ' %s' % (data.data))


	def listener():
		rospy.init_node('listener', anonymous=True)
		rospy.Subscriber('chatter', String, listen_callback)
		rospy.spin()


	if __name__ == '__main__':
		listener()
snippet ros_client
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-

	import sys
	import rospy
	from std_srvs.srv import SetBool


	def set_bool_command(b):
		rospy.wait_for_service('set_bool')
		try:
			set_bool = rospy.ServiceProxy('set_bool', SetBool)
			res = set_bool(b)
			return res.success, res.message
		except rospy.ServiceException as e:
			print('Service Call Failed: %s' % (e))


	def main():
		success, message = set_bool_command(True)
		print('%s, %s' % (success, message))


	if __name__ == '__main__':
		main()
snippet ros_server
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-

	from std_srvs.srv import SetBool, SetBoolResponse
	import rospy


	def set_bool(req):
		return SetBoolResponse(req.data, 'success')


	def start_server():
		rospy.init_node('set_bool_server')
		rospy.Service('set_bool', SetBool, set_bool)
		print('Server Ready...')
		rospy.spin()


	if __name__ == '__main__':
		start_server()
snippet ros_smach_state_machine
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-

	import roslib
	import rospy
	import smach
	import smach_ros


	class ${2:State1}(smach.State):
		def __init__(self):
			smach.State.__init__(self, outcomes=['outcome1', 'outcome2'],
								input_keys=['counter_in'],
								output_keys=['counter_out'])

		def execute(self, userdata):
			rospy.loginfo('Executing state 1')
			if userdata.counter_in < 3:
				userdata.counter_out = userdata.counter_in + 1
				return 'outcome1'
			else:
				return 'outcome2'


	class ${3:State2}(smach.State):
		def __init__(self):
			smach.State.__init__(self, outcomes=['outcome2'],
								input_keys=['counter_in'])

		def execute(self, userdata):
			rospy.loginfo('Executing state 2')
			rospy.loginfo('Counter: %f' % (userdata.counter_in))
			return 'outcome1'


	def main():
		rospy.init_node('${1:smach_state_machine}')
		sm = smach.StateMachine(outcomes=['outcome4'])
		sm.userdata.sm_counter = 0

		with sm:
			smach.StateMachine.add('$2', $2(),
								transitions={'outcome1': '$2', 'outcome2':
												'outcome4'},
								remapping={'counter_in': 'sm_counter',
											'counter_out': 'sm_counter'})
			smach.StateMachine.add('$3', $3(),
								transitions={'outcome2': '$3'},
								remapping={'counter_in': 'sm_counter'})

		outcome = sm.execute()
		print(outcome)


	if __name__ == '__main__':
		main()
##
## scripts
snippet script_mnist_train_sqlite
	import sqlite3
	import pandas as pd
	import numpy as np


	data_frame = pd.read_csv('./train.csv').as_matrix()

	connection = sqlite3.connect('mnist.sqlite3')
	cursor = connection.cursor()

	cursor.execute("""CREATE TABLE IF NOT EXISTS mnist(
	image BLOB NOT NULL,
	width INTEGER NOT NULL,
	height INTEGER NOT NULL,
	label INTEGER NOT NULL);""")


	for i in range(len(data_frame)):
		label = data_frame[i, 0]
		image = data_frame[i, 1:].astype(np.uint8)
		cursor.execute("""INSERT INTO mnist VALUES(?, ?, ?, ?)""",
			[buffer(image), 28, 28, label])

	connection.commit()
	connection.close()
snippet script_keras_train
	import sqlite3
	import numpy as np
	import os
	import logging

	import keras
	from keras.models import Sequential
	from keras.layers import Dense, Activation


	logging.basicConfig()
	logger = logging.getLogger('mnist')
	logger.setLevel(logging.INFO)



	def load_data(db_path):
		data = []
		label = []

		if os.path.isfile(db_path):
			logger.info('loading %s...', db_path)

			connection = sqlite3.connect(db_path)
			cursor = connection.cursor()

			cursor.execute("""SELECT * FROM mnist;""")
			raw_data = cursor.fetchall()
			for entry in raw_data:
				image = np.frombuffer(entry[0], np.uint8)
				image = image.reshape([entry[2], entry[1]])
				data.append(image)
				label.append(entry[-1])

			connection.commit()
			connection.close()
		else:
			logger.error('fail to find %s.', db_path)

	return np.array(data), np.array(label)


	def build_model():
		model = Sequential()
		model.add(Dense(256, input_dim=784,
			kernel_initializer=keras.initializers.RandomNormal(stddev=0.02)))
		model.add(Activation('relu'))
		model.add(Dense(10))
		model.add(Activation('softmax'))
		model.compile(loss='categorical_crossentropy',
			optimizer=keras.optimizers.SGD(lr=1e-4, momentum=0.9),
			metrics=['accuracy'])
	return model


	def main():
		data, label = load_data('./mnist.sqlite3')
		data = data.reshape([-1, 28 * 28])
		label = np.eye(10)[label]

		model = build_model()
		model.fit(data, label, epochs=100, batch_size=256)


	if __name__ == '__main__':
		main()
##
##
snippet script_replay_buffer
	import numpy as np
	import random
	from collections import deque


	class ReplayBuffer(object):
		def __init__(self, replay_buffer_size, image_width, image_height,
				history_size):
			self.w, self.h = image_width, image_height
			self.size = replay_buffer_size
			self.history_size = history_size
			self.history = deque(maxlen=history_size - 1)
			self._state = np.zeros(shape=[replay_buffer_size, image_height,
				image_width], dtype=np.uint8)
			self._action = np.zeros(shape=[replay_buffer_size], dtype=np.int32)
			self._reward = np.zeros(shape=[replay_buffer_size], dtype=np.float32)
			self._done = np.zeros(shape=[replay_buffer_size], dtype=np.bool)
			self._current_index = 0
			self._current_size = 0

		def _add(self, index, state, action, reward, done):
			self._state[index] = state
			self._reward[index] = reward
			self._action[index] = action
			self._done[index] = done

		def add(self, state, action, reward, done):
			self._add(self._current_index, state, action, reward, done)
			self._current_index = (self._current_index + 1) % self.size
			self._current_size = min(self._current_size + 1, self.size)

			if done:
				self.history.clear()
			else:
				self.history.append(state)

		def recent_state(self, latest_state):
			recent = list(self.history)
			states = [np.zeros([self.h, self.w], np.uint8)] * \
				(self.history.maxlen - len(recent))
			states.extend([state for state in recent])
			states.append(latest_state)
			return np.stack(states, axis=2)

		@property
		def current_size(self):
			return self._current_size

		def _slice(self, data, start, end):
			a1 = data[start:]
			a2 = data[:end]
			return np.concatenate((a1, a2), axis=0)

		def _pad(self, state, reward, action, done):
			for k in range(self.history_size - 2, -1, -1):
				if done[k]:
					state = np.copy(state)
					state[:k + 1].fill(0)
					break
			state = state.transpose(1, 2, 0)
			return state[:, :, 0:self.history_size], action[-2], \
				state[:, :, 1:], reward[-2], done[-2]

		def _sample(self, index):
			index = (self._current_index + index) % self._current_size
			k = self.history_size + 1

			if index + k <= self._current_size:
				state = self._state[index:(index + k)]
				reward = self._reward[index:(index + k)]
				action = self._action[index:(index + k)]
				done = self._done[index:(index + k)]
			else:
				end = index + k - self._current_size
				state = self._slice(self._state, index, end)
				reward = self._slice(self._reward, index, end)
				action = self._slice(self._action, index, end)
				done = self._slice(self._done, index, end)
				sampled = self._pad(state, reward, action, done)
			return sampled

		def _process_batch(self, batch):
			states, actions, next_states, rewards, done = batch
			states = np.asarray(states, dtype=np.uint8)
			actions = np.asarray(actions, dtype=np.int8)
			next_states = np.asarray(next_states, dtype=np.uint8)
			rewards = np.asarray(rewards, dtype=np.float32)
			done = np.asarray(done, dtype=np.bool)
			return states, actions, next_states, rewards, done

		def sample(self, batch_size):
			indices = np.random.randint(0, self._current_size - self.history_size - 1,
				[batch_size])
			batch = zip(*[self._sample(i) for i in indices])
			return self._process_batch(batch)
