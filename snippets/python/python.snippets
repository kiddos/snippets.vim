snippet #!
	#!/usr/bin/env python
	# -*- coding: utf-8 -*-
snippet #!3
	#!/usr/bin/env python3
	# -*- coding: utf-8 -*-
snippet unicode
	def __unicode__(self):
		${0:representation}
snippet docs
	"""
	File: ${1:`vim_snippets#Filename('$1.py', 'foo.py')`}
	Author: Joseph
	Email: kiddo831007@gmail.com
	Github: https://github.com/kiddos
	Description: ${0}
	"""

snippet edocs
	"""${1:Description}

	@param ${2:param}: ${3: Description}
	@type  $2: ${4: Type}

	@return: ${5: Description}
	@rtype : ${6: Type}

	@raise e: ${0: Description}
	"""
# Encodes
snippet encoding_utf8
	# -*- coding: utf-8 -*-
snippet encoding_latin1
	# -*- coding: latin-1 -*-
snippet encoding_ascii
	# -*- coding: ascii -*-
snippet main
	def main():
		${1:pass}


	if __name__ == '__main__':
		${0:main()}
snippet ifmain
	if __name__ == '__main__':
		${0:main()}
# logging
snippet logging
	import logging
	logger = logging.getLogger(${0:__name__})

snippet logger
	logging.basicConfig()
	logger = logging.getLogger(${1:__name__})
	logger.setLevel(${2:logging.INFO})

# argument parser
snippet argparse
	from argparse import ArgumentParser

	parser = ArgumentParser()
	${0}
	args = parser.parse_args()
## defined classes template
snippet class_unit_test
	class TestName(unittest.TestCase):
		def test_case1(self):
			self.assertTrue(True)
##
## Serial
snippet serial
	import serial

	s = serial.Serial('${1:/dev/ttyUSB0}', ${2:9600},
		parity=${2:serial.PARITY_EVEN},
		stopbits=${3:serial.STOPBITS_ONE},
		timeout=${4:1000})
##
## scripts
snippet script_mnist_train_sqlite
	import sqlite3
	import pandas as pd
	import numpy as np


	data_frame = pd.read_csv('./train.csv').as_matrix()

	connection = sqlite3.connect('mnist.sqlite3')
	cursor = connection.cursor()

	cursor.execute("""CREATE TABLE IF NOT EXISTS mnist(
	image BLOB NOT NULL,
	width INTEGER NOT NULL,
	height INTEGER NOT NULL,
	label INTEGER NOT NULL);""")


	for i in range(len(data_frame)):
		label = data_frame[i, 0]
		image = data_frame[i, 1:].astype(np.uint8)
		cursor.execute("""INSERT INTO mnist VALUES(?, ?, ?, ?)""",
			[buffer(image), 28, 28, label])

	connection.commit()
	connection.close()
snippet script_replay_buffer
	import numpy as np
	import random
	from collections import deque


	class ReplayBuffer(object):
		def __init__(self, replay_buffer_size, image_width, image_height,
				history_size):
			self.w, self.h = image_width, image_height
			self.size = replay_buffer_size
			self.history_size = history_size
			self.history = deque(maxlen=history_size - 1)
			self._state = np.zeros(shape=[replay_buffer_size, image_height,
				image_width], dtype=np.uint8)
			self._action = np.zeros(shape=[replay_buffer_size], dtype=np.int32)
			self._reward = np.zeros(shape=[replay_buffer_size], dtype=np.float32)
			self._done = np.zeros(shape=[replay_buffer_size], dtype=np.bool)
			self._current_index = 0
			self._current_size = 0

		def _add(self, index, state, action, reward, done):
			self._state[index] = state
			self._reward[index] = reward
			self._action[index] = action
			self._done[index] = done

		def add(self, state, action, reward, done):
			self._add(self._current_index, state, action, reward, done)
			self._current_index = (self._current_index + 1) % self.size
			self._current_size = min(self._current_size + 1, self.size)

			if done:
				self.history.clear()
			else:
				self.history.append(state)

		def recent_state(self, latest_state):
			recent = list(self.history)
			states = [np.zeros([self.h, self.w], np.uint8)] * \
				(self.history.maxlen - len(recent))
			states.extend([state for state in recent])
			states.append(latest_state)
			return np.stack(states, axis=2)

		@property
		def current_size(self):
			return self._current_size

		def _slice(self, data, start, end):
			a1 = data[start:]
			a2 = data[:end]
			return np.concatenate((a1, a2), axis=0)

		def _pad(self, state, reward, action, done):
			for k in range(self.history_size - 2, -1, -1):
				if done[k]:
					state = np.copy(state)
					state[:k + 1].fill(0)
					break
			state = state.transpose(1, 2, 0)
			return state[:, :, 0:self.history_size], action[-2], \
				state[:, :, 1:], reward[-2], done[-2]

		def _sample(self, index):
			index = (self._current_index + index) % self._current_size
			k = self.history_size + 1

			if index + k <= self._current_size:
				state = self._state[index:(index + k)]
				reward = self._reward[index:(index + k)]
				action = self._action[index:(index + k)]
				done = self._done[index:(index + k)]
			else:
				end = index + k - self._current_size
				state = self._slice(self._state, index, end)
				reward = self._slice(self._reward, index, end)
				action = self._slice(self._action, index, end)
				done = self._slice(self._done, index, end)
				sampled = self._pad(state, reward, action, done)
			return sampled

		def _process_batch(self, batch):
			states, actions, next_states, rewards, done = batch
			states = np.asarray(states, dtype=np.uint8)
			actions = np.asarray(actions, dtype=np.int8)
			next_states = np.asarray(next_states, dtype=np.uint8)
			rewards = np.asarray(rewards, dtype=np.float32)
			done = np.asarray(done, dtype=np.bool)
			return states, actions, next_states, rewards, done

		def sample(self, batch_size):
			indices = np.random.randint(0, self._current_size - self.history_size - 1,
				[batch_size])
			batch = zip(*[self._sample(i) for i in indices])
			return self._process_batch(batch)
snippet script_experience_replay
	import tensorflow as tf
	import numpy as np
	import threading
	import random
	import unittest
	import time
	from collections import deque

	from run import load_graph
	from replay_buffer import ReplayBuffer
	from environment import get_training_env


	class ExperienceReplay(object):
		def __init__(self, env_name,
				replay_buffer_size, w, h, history_size, policy, decay_steps):
			self.epsilon = 1.0
			self.decay_steps = decay_steps
			self.policy = policy
			self.steps = 0

			self.replay_buffer = ReplayBuffer(replay_buffer_size, w, h, history_size)
			self.env = get_training_env(env_name, w, h)
			self.ave_rewards = deque(maxlen=100)
			self.max_reward = 0

		def run(self):
			self.running = True
			while self.running:
				state = self.env.reset()
				total_reward = 0
				while self.running:
					action = self.epsilon_greedy(state)
					next_state, reward, done, info = self.env.step(action)
					self.replay_buffer.add(state, action, reward, done)
					state = next_state
					total_reward += reward

					if done:
						self.ave_rewards.append(total_reward)
						if total_reward >= self.max_reward:
							self.max_reward = total_reward
						break

		def init_replay_buffer(self, size):
			while self.replay_buffer.current_size < size:
				state = self.env.reset()
				while True:
					action = self.epsilon_greedy(state)
					next_state, reward, done, info = self.env.step(action)
					self.replay_buffer.add(state, action, reward, done)
					state = next_state
					if done: break

		def set_epsilon(self, epsilon):
			self.epsilon = epsilon

		def start(self):
			self.task = threading.Thread(target=self.run)
			self.task.start()

		def stop(self):
			self.running = False
			if hasattr(self, 'task'):
				self.task.join()

		@property
		def average_reward(self):
			return sum(self.ave_rewards) / len(self.ave_rewards)

		@property
		def max_ave_reward(self):
			return max(self.ave_rewards)

		def epsilon_greedy(self, state):
			if random.random() < self.epsilon:
				return self.env.action_space.sample()
			else:
				return self.policy(self.replay_buffer.recent_state(state))
snippet import_unit_test
	import unittest
	from unittest import TestCase
